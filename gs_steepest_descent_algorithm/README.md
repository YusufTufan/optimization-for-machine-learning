# Steepest Descent Method with Golden Section Line Search

This repository implements the **Steepest Descent Algorithm** combined with **Golden Section Line Search** to solve unconstrained nonlinear optimization problems more efficiently.

## ğŸ“Œ Problem Definition

The goal is to minimize a differentiable scalar function `f(x)` without constraints by moving in the direction of steepest descent (i.e., negative gradient) and determining the optimal step size using golden section search.

Mathematically:

```math
\min_{x \in \mathbb{R}^n} f(x)
````

## ğŸ” Method Overview

**Steepest Descent Method** is a first-order iterative optimization technique that updates the current solution by moving along the direction of the negative gradient.

To improve convergence, the step size (learning rate) at each iteration is computed using **Golden Section Search**, instead of using a fixed or arbitrary value.

### Key Features:

* Derivative-based method (requires gradient)
* Direction: `dâ‚– = -âˆ‡f(xâ‚–)`
* Step size `Î±â‚–` is chosen via golden section line search
* Suitable for convex or smooth non-convex functions

## âš™ï¸ Algorithm Steps

1. Initialize `xâ‚€`
2. While stopping criteria not met:
   a. Compute gradient `âˆ‡f(xâ‚–)`
   b. Set direction `dâ‚– = -âˆ‡f(xâ‚–)`
   c. Use Golden Section Search to find optimal `Î±â‚–` along `dâ‚–`
   d. Update position: `xâ‚–â‚Šâ‚ = xâ‚– + Î±â‚– * dâ‚–`
3. Return the best solution found

---

# AltÄ±n BÃ¶lme ile En Dik Ä°niÅŸ (Steepest Descent) YÃ¶ntemi

Bu repoda, **En Dik Ä°niÅŸ (Steepest Descent)** algoritmasÄ± ve **AltÄ±n BÃ¶lme (Golden Section)** Ã§izgisel arama yÃ¶ntemi birlikte kullanÄ±larak tÃ¼revlenebilir fonksiyonlarÄ±n daha verimli optimize edilmesi amaÃ§lanmaktadÄ±r.

## ğŸ“Œ Problem TanÄ±mÄ±

AmaÃ§, herhangi bir kÄ±sÄ±t olmaksÄ±zÄ±n, tÃ¼revlenebilir bir skaler `f(x)` fonksiyonunu minimize etmektir. Bu, en hÄ±zlÄ± azalma yÃ¶nÃ¼ olan negatif gradyan doÄŸrultusunda ilerleyerek ve adÄ±m bÃ¼yÃ¼klÃ¼ÄŸÃ¼nÃ¼ altÄ±n oranla belirleyerek gerÃ§ekleÅŸtirilir.

Matematiksel ifade:

```math
\min_{x \in \mathbb{R}^n} f(x)
```

## ğŸ” YÃ¶ntem Ã–zeti

**En Dik Ä°niÅŸ YÃ¶ntemi**, mevcut Ã§Ã¶zÃ¼mÃ¼, fonksiyonun negatif gradyanÄ± yÃ¶nÃ¼nde gÃ¼ncelleyerek iteratif olarak en iyi Ã§Ã¶zÃ¼me yaklaÅŸÄ±r.

Bu uygulamada, her adÄ±mda adÄ±m bÃ¼yÃ¼klÃ¼ÄŸÃ¼ (`Î±â‚–`) sabit deÄŸil, **AltÄ±n BÃ¶lme AramasÄ±** ile en iyi deÄŸer olarak belirlenir.

### Temel Ã–zellikler:

* Gradyan tabanlÄ± bir optimizasyon yÃ¶ntemidir
* YÃ¶n: `dâ‚– = -âˆ‡f(xâ‚–)`
* AdÄ±m bÃ¼yÃ¼klÃ¼ÄŸÃ¼: AltÄ±n BÃ¶lme yÃ¶ntemiyle hesaplanÄ±r
* Konveks ve dÃ¼zgÃ¼n doÄŸrusal olmayan fonksiyonlar iÃ§in uygundur

## âš™ï¸ Algoritma AdÄ±mlarÄ±

1. BaÅŸlangÄ±Ã§ noktasÄ± `xâ‚€` seÃ§ilir
2. Durma kriteri saÄŸlanana kadar:
   a. Gradyan `âˆ‡f(xâ‚–)` hesaplanÄ±r
   b. YÃ¶n `dâ‚– = -âˆ‡f(xâ‚–)` olarak belirlenir
   c. Bu yÃ¶n boyunca optimum `Î±â‚–`, AltÄ±n BÃ¶lme ile hesaplanÄ±r
   d. GÃ¼ncelleme: `xâ‚–â‚Šâ‚ = xâ‚– + Î±â‚– * dâ‚–`
3. En iyi bulunan Ã§Ã¶zÃ¼m dÃ¶ndÃ¼rÃ¼lÃ¼r

