# Davidonâ€“Fletcherâ€“Powell (DFP) Algorithm for Unconstrained Optimization

This repository includes a Python implementation of the **DFP algorithm**, a quasi-Newton method used to solve unconstrained nonlinear optimization problems by approximating the inverse Hessian matrix.

## ğŸ“Œ Problem Definition

The objective is to minimize a multivariable, continuously differentiable function `f(x)` using only first-order derivative information.

Mathematically:

```math
\min f(x), \quad x \in \mathbb{R}^n
````

## ğŸ” Method Overview

The **DFP algorithm** is one of the earliest quasi-Newton methods. Instead of computing the Hessian matrix directly, it maintains and updates an approximation of the **inverse Hessian** in each iteration using gradient differences and step vectors.

### Core Concepts:

* Iterative approximation of inverse Hessian matrix `Hâ‚–`
* Uses gradient vectors `âˆ‡fâ‚–` at each step
* Update rule uses `sâ‚– = xâ‚–â‚Šâ‚ - xâ‚–` and `yâ‚– = âˆ‡fâ‚–â‚Šâ‚ - âˆ‡fâ‚–`

**Update formula**:

```math
H_{k+1} = H_k + \frac{s_k s_k^T}{s_k^T y_k} - \frac{H_k y_k y_k^T H_k}{y_k^T H_k y_k}
```

**Advantages**:

* Faster convergence than steepest descent
* No need to compute the true Hessian
* Suitable for large-scale optimization problems

## âš™ï¸ Algorithm Steps

1. Initialize starting point `xâ‚€` and set `Hâ‚€ = I` (identity matrix)
2. While not converged:

   * Compute gradient `âˆ‡fâ‚–`
   * Determine search direction `dâ‚– = -Hâ‚– * âˆ‡fâ‚–`
   * Perform line search to compute step size `Î±â‚–`
   * Update position: `xâ‚–â‚Šâ‚ = xâ‚– + Î±â‚– * dâ‚–`
   * Compute `sâ‚– = xâ‚–â‚Šâ‚ - xâ‚–`, `yâ‚– = âˆ‡fâ‚–â‚Šâ‚ - âˆ‡fâ‚–`
   * Update inverse Hessian approximation `Hâ‚–` using DFP formula

---

# Davidonâ€“Fletcherâ€“Powell (DFP) AlgoritmasÄ± ile KÄ±sÄ±tsÄ±z Optimizasyon

Bu repoda, **DFP algoritmasÄ±nÄ±n** Python ile gerÃ§ekleÅŸtirilmiÅŸ bir uygulamasÄ± bulunmaktadÄ±r. DFP, Hessian matrisini yaklaÅŸÄ±k olarak hesaplayarak **kÄ±sÄ±tsÄ±z doÄŸrusal olmayan fonksiyonlarÄ±n** minimizasyonunda kullanÄ±lan bir **quasi-Newton yÃ¶ntemidir**.

## ğŸ“Œ Problem TanÄ±mÄ±

AmaÃ§, sÃ¼rekli tÃ¼revlenebilir Ã§ok deÄŸiÅŸkenli bir `f(x)` fonksiyonunun minimumunu, yalnÄ±zca birinci tÃ¼rev bilgisi kullanarak bulmaktÄ±r.

Matematiksel ifade:

```math
\min f(x), \quad x \in \mathbb{R}^n
```

## ğŸ” YÃ¶ntem Ã–zeti

**DFP algoritmasÄ±**, en eski quasi-Newton yÃ¶ntemlerinden biridir. GerÃ§ek Hessian matrisini doÄŸrudan hesaplamak yerine, onun tersinin yaklaÅŸÄ±k deÄŸerini iteratif olarak gÃ¼nceller. Bu iÅŸlem, gradyan ve adÄ±m farklarÄ±na dayanÄ±r.

### Temel Kavramlar:

* `Hâ‚–` ile ters Hessian matrisi yaklaÅŸÄ±k olarak tutulur
* Her adÄ±mda gradyan `âˆ‡fâ‚–` kullanÄ±lÄ±r
* GÃ¼ncelleme: `sâ‚– = xâ‚–â‚Šâ‚ - xâ‚–`, `yâ‚– = âˆ‡fâ‚–â‚Šâ‚ - âˆ‡fâ‚–`

**GÃ¼ncelleme formÃ¼lÃ¼**:

```math
H_{k+1} = H_k + \frac{s_k s_k^T}{s_k^T y_k} - \frac{H_k y_k y_k^T H_k}{y_k^T H_k y_k}
```

**AvantajlarÄ±**:

* En dik iniÅŸe gÃ¶re daha hÄ±zlÄ± yakÄ±nsar
* GerÃ§ek Hessian matrisine ihtiyaÃ§ duymaz
* BÃ¼yÃ¼k Ã¶lÃ§ekli problemlerde kullanÄ±labilir

## âš™ï¸ Algoritma AdÄ±mlarÄ±

1. BaÅŸlangÄ±Ã§ noktasÄ± `xâ‚€` ve baÅŸlangÄ±Ã§ matris `Hâ‚€ = I` seÃ§ilir
2. YakÄ±nsama saÄŸlanana kadar:

   * Gradyan `âˆ‡fâ‚–` hesaplanÄ±r
   * Arama yÃ¶nÃ¼: `dâ‚– = -Hâ‚– * âˆ‡fâ‚–`
   * Line search ile `Î±â‚–` adÄ±mÄ± hesaplanÄ±r
   * Yeni nokta: `xâ‚–â‚Šâ‚ = xâ‚– + Î±â‚– * dâ‚–`
   * `sâ‚– = xâ‚–â‚Šâ‚ - xâ‚–`, `yâ‚– = âˆ‡fâ‚–â‚Šâ‚ - âˆ‡fâ‚–` hesaplanÄ±r
   * DFP formÃ¼lÃ¼yle `Hâ‚–` gÃ¼ncellenir
