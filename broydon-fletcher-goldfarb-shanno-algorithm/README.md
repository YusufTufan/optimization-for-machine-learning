# Broydenâ€“Fletcherâ€“Goldfarbâ€“Shanno (BFGS) Algorithm for Unconstrained Optimization

This repository contains a Python implementation of the **BFGS algorithm**, one of the most widely used **quasi-Newton methods** for solving unconstrained nonlinear optimization problems.

## ğŸ“Œ Problem Definition

The BFGS algorithm aims to find the local minimum of a differentiable, unconstrained multivariable function `f(x)`. It does so by iteratively updating an estimate of the inverse Hessian matrix using gradient evaluations.

Mathematical Objective:

```math
\min f(x), \quad x \in \mathbb{R}^n
````

## ğŸ” Method Overview

The BFGS algorithm is a **quasi-Newton method** that avoids computing the full Hessian matrix, making it computationally efficient. Instead, it builds up an approximation of the inverse Hessian matrix using gradient differences and step sizes.

**Core ideas**:

* Uses gradient vectors âˆ‡f(x)
* Approximates the inverse of the Hessian matrix
* Updates direction using: `p_k = -H_k * âˆ‡f_k`
* Performs line search along that direction
* Updates `H_k` using the BFGS formula

**Advantages**:

* Superlinear convergence
* No need to compute second derivatives
* More stable and accurate than DFP or steepest descent

## âš™ï¸ Algorithm Steps

1. Initialize `xâ‚€`, `Hâ‚€ = I` (identity matrix)
2. While not converged:

   * Compute gradient `âˆ‡f_k`
   * Compute search direction `p_k = -H_k * âˆ‡f_k`
   * Perform line search to find optimal step size `Î±_k`
   * Update position: `x_{k+1} = x_k + Î±_k * p_k`
   * Compute `s_k = x_{k+1} - x_k`, `y_k = âˆ‡f_{k+1} - âˆ‡f_k`
   * Update `H_k` using BFGS formula

# Broydonâ€“Fletcherâ€“Goldfarbâ€“Shanno (BFGS) AlgoritmasÄ± ile KÄ±sÄ±tsÄ±z Optimizasyon

Bu repoda, **BFGS algoritmasÄ±nÄ±n** Python ile gerÃ§ekleÅŸtirilmiÅŸ bir uygulamasÄ± bulunmaktadÄ±r. Bu yÃ¶ntem, **kÄ±sÄ±tsÄ±z doÄŸrusal olmayan Ã§ok deÄŸiÅŸkenli fonksiyonlarÄ±n** optimizasyonu iÃ§in yaygÄ±n olarak kullanÄ±lan bir **quasi-Newton yÃ¶ntemidir**.

## ğŸ“Œ Problem TanÄ±mÄ±

BFGS algoritmasÄ±nÄ±n amacÄ±, tÃ¼revlenebilir bir Ã§ok deÄŸiÅŸkenli fonksiyonun yerel minimumunu bulmaktÄ±r. Bunu, Hessian matrisinin tersini yaklaÅŸÄ±k olarak hesaplayarak ve gradyan bilgilerini kullanarak iteratif ÅŸekilde yapar.

Matematiksel ifade:

```math
\min f(x), \quad x \in \mathbb{R}^n
```

## ğŸ” YÃ¶ntem Ã–zeti

BFGS, tam Hessian matrisini hesaplamadan Ã§alÄ±ÅŸan bir **quasi-Newton yÃ¶ntemidir**. Gradyanlar ve adÄ±m farklarÄ±nÄ± kullanarak ters Hessian matrisin yaklaÅŸÄ±k deÄŸerini gÃ¼nceller.

**Temel prensipler**:

* Gradyan vektÃ¶rlerini (âˆ‡f) kullanÄ±r
* Hessian matrisin tersini yaklaÅŸÄ±k hesaplar
* YÃ¶n: `p_k = -H_k * âˆ‡f_k` ile hesaplanÄ±r
* Bu yÃ¶nde bir adÄ±m bÃ¼yÃ¼klÃ¼ÄŸÃ¼ (line search) ile ilerlenir
* `H_k`, BFGS gÃ¼ncelleme formÃ¼lÃ¼yle gÃ¼ncellenir

**AvantajlarÄ±**:

* SÃ¼perlineer yakÄ±nsama
* Ä°kinci tÃ¼rev hesaplamasÄ± gerektirmez
* DFP veya en dik iniÅŸ yÃ¶ntemine gÃ¶re daha kararlÄ± ve doÄŸrudur

## âš™ï¸ Algoritma AdÄ±mlarÄ±

1. BaÅŸlangÄ±Ã§ vektÃ¶rÃ¼ `xâ‚€` ve `Hâ‚€ = I` olarak belirlenir
2. YakÄ±nsama saÄŸlanana kadar:

   * Gradyan `âˆ‡f_k` hesaplanÄ±r
   * YÃ¶n: `p_k = -H_k * âˆ‡f_k`
   * Uygun adÄ±m bÃ¼yÃ¼klÃ¼ÄŸÃ¼ `Î±_k` iÃ§in line search yapÄ±lÄ±r
   * Konum gÃ¼ncellenir: `x_{k+1} = x_k + Î±_k * p_k`
   * `s_k = x_{k+1} - x_k`, `y_k = âˆ‡f_{k+1} - âˆ‡f_k`
   * `H_k`, BFGS formÃ¼lÃ¼ ile gÃ¼ncellenir
