# Levenbergâ€“Marquardt Algorithm for Nonlinear Least Squares Optimization

This repository provides an implementation of the **Levenbergâ€“Marquardt (LM)** algorithm, a widely used optimization technique for solving nonlinear least squares problems.

## ğŸ“Œ Problem Definition

The goal is to minimize the sum of squared residuals between a nonlinear model `f(x, t)` and observed data `y(t)`:

```math
\min_{x} \sum_{i=1}^{N} \left(y_i - f(x, t_i)\right)^2
````

Here, `x` is the vector of model parameters, and `f(x, t)` is the nonlinear function being fitted.

## ğŸ” Method Overview

The **Levenbergâ€“Marquardt Algorithm** combines the advantages of:

* **Gradient Descent**: robust but slow
* **Gaussâ€“Newton**: fast near the solution but unstable if far

LM dynamically balances between these two depending on the current error, via a damping factor `Î»`.

### Key Characteristics:

* Used in nonlinear curve fitting and machine learning
* Adaptive update using Jacobian matrix `J`
* Well-suited for overdetermined systems (more equations than unknowns)

### Update Rule:

```math
x_{k+1} = x_k + \left(J^T J + \lambda I\right)^{-1} J^T r
```

Where:

* `J` is the Jacobian matrix of partial derivatives
* `r` is the residual vector: `r = y - f(x)`
* `Î»` controls the blend between gradient descent and Gaussâ€“Newton

## âš™ï¸ Algorithm Steps

1. Initialize parameter vector `xâ‚€`, damping factor `Î»`, and tolerance
2. While stopping criteria not met:

   * Compute residuals `r = y - f(x)`
   * Compute Jacobian matrix `J`
   * Update `x` using LM update rule
   * Adjust `Î»` based on error improvement
3. Return optimized parameters `x`


# DoÄŸrusal Olmayan En KÃ¼Ã§Ã¼k Kareler Optimizasyonu iÃ§in Levenbergâ€“Marquardt AlgoritmasÄ±

Bu repoda, **Levenbergâ€“Marquardt (LM)** algoritmasÄ±nÄ±n bir uygulamasÄ± yer almaktadÄ±r. Bu yÃ¶ntem, doÄŸrusal olmayan en kÃ¼Ã§Ã¼k kareler problemlerini Ã§Ã¶zmek iÃ§in yaygÄ±n olarak kullanÄ±lÄ±r.

## ğŸ“Œ Problem TanÄ±mÄ±

AmaÃ§, doÄŸrusal olmayan bir `f(x, t)` model fonksiyonu ile gÃ¶zlemler `y(t)` arasÄ±ndaki hatalarÄ±n karesel toplamÄ±nÄ± minimize etmektir:

```math
\min_{x} \sum_{i=1}^{N} \left(y_i - f(x, t_i)\right)^2
````

Burada `x`, optimize edilecek parametre vektÃ¶rÃ¼dÃ¼r ve `f(x, t)` tahmin fonksiyonudur.

## ğŸ” YÃ¶ntem Ã–zeti

**Levenbergâ€“Marquardt AlgoritmasÄ±**, iki yÃ¶ntemin avantajlarÄ±nÄ± birleÅŸtirir:

* **Gradyan iniÅŸi**: KararlÄ±dÄ±r ancak yavaÅŸtÄ±r
* **Gaussâ€“Newton**: HÄ±zlÄ±dÄ±r ancak baÅŸlangÄ±Ã§tan uzaksa dengesiz olabilir

LM yÃ¶ntemi, bu iki yaklaÅŸÄ±mÄ± `Î»` adÄ± verilen bir sÃ¶nÃ¼mleme katsayÄ±sÄ± ile dengeler.

### Temel Ã–zellikler:

* DoÄŸrusal olmayan eÄŸri uydurma ve makine Ã¶ÄŸrenmesi uygulamalarÄ±nda kullanÄ±lÄ±r
* Jacobian matrisi `J` ile hesaplama yapÄ±lÄ±r
* Bilinmeyenden fazla denklem (overdetermined) olan sistemler iÃ§in uygundur

### GÃ¼ncelleme FormÃ¼lÃ¼:

```math
x_{k+1} = x_k + \left(J^T J + \lambda I\right)^{-1} J^T r
```

Burada:

* `J`, modelin tÃ¼revlerinden oluÅŸan Jacobian matrisidir
* `r`, artÄ±k (residual) vektÃ¶rÃ¼dÃ¼r: `r = y - f(x)`
* `Î»`, yÃ¶ntemin ne kadar Gaussâ€“Newton veya gradyan iniÅŸ gibi Ã§alÄ±ÅŸacaÄŸÄ±nÄ± belirler

## âš™ï¸ Algoritma AdÄ±mlarÄ±

1. Parametre vektÃ¶rÃ¼ `xâ‚€`, sÃ¶nÃ¼mleme katsayÄ±sÄ± `Î»` ve tolerans tanÄ±mlanÄ±r
2. Durma koÅŸulu saÄŸlanana kadar:

   * ArtÄ±klar `r = y - f(x)` hesaplanÄ±r
   * Jacobian matrisi `J` hesaplanÄ±r
   * GÃ¼ncelleme formÃ¼lÃ¼ ile `x` yeni deÄŸeri bulunur
   * Hata azalmasÄ±na gÃ¶re `Î»` gÃ¼ncellenir
3. Optimize edilmiÅŸ parametreler `x` dÃ¶ndÃ¼rÃ¼lÃ¼r
