# Gradient Descent Algorithm for Unconstrained Optimization

This repository contains a Python implementation of the **Gradient Descent Algorithm**, a fundamental first-order optimization technique used to minimize differentiable scalar functions.

## ğŸ“Œ Problem Definition

The objective is to minimize a continuously differentiable function `f(x)` over â„â¿ using only first-order derivative (gradient) information.

Mathematically:

```math
\min_{x \in \mathbb{R}^n} f(x)
````

## ğŸ” Method Overview

**Gradient Descent** is an iterative method that updates the solution in the direction of the negative gradient of the function, which points toward the steepest descent.

### Key Concepts:

* Uses the gradient `âˆ‡f(x)` to guide updates
* Does not require second derivatives
* Step size (learning rate) `Î±` is crucial for convergence

### Update Rule:

```math
x_{k+1} = x_k - \alpha \nabla f(x_k)
```

* A small `Î±` causes slow convergence
* A large `Î±` may overshoot or diverge
* `Î±` may be fixed or determined via line search

## âš™ï¸ Algorithm Steps

1. Choose initial guess `xâ‚€`
2. Set learning rate `Î±` and tolerance `Îµ`
3. While `â€–âˆ‡f(xâ‚–)â€– > Îµ`:

   * Compute gradient `âˆ‡f(xâ‚–)`
   * Update: `xâ‚–â‚Šâ‚ = xâ‚– - Î± âˆ‡f(xâ‚–)`
4. Return final `x` as the estimated minimum

---

# KÄ±sÄ±tsÄ±z Optimizasyon iÃ§in Gradyan Ä°niÅŸi (Gradient Descent) AlgoritmasÄ±

Bu repoda, sÃ¼rekli tÃ¼revlenebilir bir skaler fonksiyonun minimumunu bulmak iÃ§in kullanÄ±lan temel bir birinci dereceden optimizasyon yÃ¶ntemi olan **Gradyan Ä°niÅŸi (Gradient Descent)** algoritmasÄ±nÄ±n Python implementasyonu yer almaktadÄ±r.

## ğŸ“Œ Problem TanÄ±mÄ±

AmaÃ§, sadece birinci tÃ¼rev bilgisi (gradyan) kullanarak sÃ¼rekli tÃ¼revlenebilir bir `f(x)` fonksiyonunu â„â¿ uzayÄ±nda minimize etmektir.

Matematiksel ifade:

```math
\min_{x \in \mathbb{R}^n} f(x)
```

## ğŸ” YÃ¶ntem Ã–zeti

**Gradyan Ä°niÅŸi** algoritmasÄ±, Ã§Ã¶zÃ¼mÃ¼, fonksiyonun en hÄ±zlÄ± azalma yÃ¶nÃ¼ olan negatif gradyan doÄŸrultusunda gÃ¼ncelleyerek iteratif olarak yaklaÅŸtÄ±rÄ±r.

### Temel Kavramlar:

* GÃ¼ncellemelerde `âˆ‡f(x)` gradyanÄ± kullanÄ±lÄ±r
* Ä°kinci tÃ¼rev bilgisine ihtiyaÃ§ duymaz
* AdÄ±m bÃ¼yÃ¼klÃ¼ÄŸÃ¼ (learning rate) `Î±`, yakÄ±nsama iÃ§in kritik Ã¶nemdedir

### GÃ¼ncelleme KuralÄ±:

```math
x_{k+1} = x_k - \alpha \nabla f(x_k)
```

* `Î±` Ã§ok kÃ¼Ã§Ã¼kse yakÄ±nsama yavaÅŸ olur
* `Î±` Ã§ok bÃ¼yÃ¼kse sapma veya patlama olabilir
* `Î±`, sabit tutulabilir ya da line search ile ayarlanabilir

## âš™ï¸ Algoritma AdÄ±mlarÄ±

1. BaÅŸlangÄ±Ã§ noktasÄ± `xâ‚€` seÃ§ilir
2. Ã–ÄŸrenme oranÄ± `Î±` ve tolerans `Îµ` belirlenir
3. `â€–âˆ‡f(xâ‚–)â€– > Îµ` olduÄŸu sÃ¼rece:

   * Gradyan `âˆ‡f(xâ‚–)` hesaplanÄ±r
   * GÃ¼ncelleme yapÄ±lÄ±r: `xâ‚–â‚Šâ‚ = xâ‚– - Î± âˆ‡f(xâ‚–)`
4. Minimum olarak `x` dÃ¶ndÃ¼rÃ¼lÃ¼r
