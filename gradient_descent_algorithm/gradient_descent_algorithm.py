
# ğŸ“‰ Gradient Descent Algorithm for Multivariable Optimization
# Bu algoritma, Ã§ok deÄŸiÅŸkenli bir fonksiyonun minimumunu bulmak iÃ§in yalnÄ±zca gradyanÄ± kullanarak iteratif olarak Ã§Ã¶zÃ¼m Ã¼retir.

import numpy as np
import matplotlib.pyplot as plt

# ğŸ¯ AmaÃ§ fonksiyonu
def f(x): 
    return 3 + (x[0] - 1.5*x[1])**2 + (x[1] - 2)**2  # Paraboloid yapÄ±lÄ± fonksiyon

# ğŸ“ˆ Gradyan (1. tÃ¼rev vektÃ¶rÃ¼)
def gradF(x):
    # Manuel tÃ¼revlenmiÅŸ hali
    return np.array([2 * (x[0] - 1.5 * x[1]), -3 * (x[0] - 1.5 * x[1]) + 2 * (x[1] - 2)])

# ğŸ“‰ Hessian (2. tÃ¼rev matrisi) - YalnÄ±zca analiz iÃ§in
def hessianF(x):
    return np.array([[2, 0], [0, 2]])

# ğŸš© BaÅŸlangÄ±Ã§ noktasÄ±
x = np.array([4, 3])
i = 0
trajectory = [x.copy()]  # AdÄ±mlarÄ±n kaydÄ±

print(f"i: {i}, f(x): {f(x):.5f}")

# ğŸ” Gradient Descent DÃ¶ngÃ¼sÃ¼
while True:
    i += 1
    x = x - 0.2 * gradF(x)  # Ã–ÄŸrenme oranÄ± = 0.2
    trajectory.append(x.copy())  # AdÄ±mÄ± kaydet

    normGradf = np.linalg.norm(gradF(x))  # Gradyan normu hesapla

    print(f"i: {i}, f(x): {f(x):.5f}, ||gradF(x)||: {normGradf:.8f}")

    if normGradf < 1e-8:  # YakÄ±nsama kriteri
        break

# ğŸ¯ Bulunan minimum noktayÄ± yazdÄ±r
print(f"x* = {x}")

# ğŸ§  NOT: Hessian matrisi Gradient Descent yÃ¶nteminde kullanÄ±lmaz.
# Ancak burada, elde edilen sonucun minimum olup olmadÄ±ÄŸÄ±nÄ± doÄŸrulamak iÃ§in kullanÄ±lmÄ±ÅŸtÄ±r.

H = hessianF(x)
ozdeger, _ = np.linalg.eig(H)

if min(ozdeger) > 0:
    print("x* noktasÄ± minimum")
elif max(ozdeger) < 0:
    print("x* noktasÄ± maksimum")
else:
    print("x* noktasÄ± semer noktasÄ±")

# ğŸ“Š Optimizasyon Yolunu GÃ¶rselleÅŸtir
trajectory = np.array(trajectory)

plt.figure(figsize=(8, 6))
plt.plot(trajectory[:, 0], trajectory[:, 1], 'r.-', label="Gradient Descent Yolu")
plt.scatter(trajectory[:, 0], trajectory[:, 1], s=30, c='blue', label="AdÄ±mlar")
plt.scatter(trajectory[0, 0], trajectory[0, 1], s=100, c='red', label="BaÅŸlangÄ±Ã§")
plt.scatter(x[0], x[1], s=100, c='green', label="Minimum")

plt.xlabel("x1")
plt.ylabel("x2")
plt.legend()
plt.title("Gradient Descent Optimizasyon Yolu")
plt.grid(True)
plt.show()
