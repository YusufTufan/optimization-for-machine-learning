# Conjugate Gradient (Fletcherâ€“Reeves) Algorithm for Unconstrained Optimization

This repository contains an implementation of the **Conjugate Gradient Method**, specifically the **Fletcherâ€“Reeves variant**, for solving unconstrained nonlinear optimization problems.

## ğŸ“Œ Problem Definition

The goal is to minimize a multivariable differentiable function `f(x)` without any constraints, using only first-order derivative (gradient) information.

Mathematically:

```math
\min f(x), \quad x \in \mathbb{R}^n
````

## ğŸ” Method Overview

The **Conjugate Gradient (CG) Method** is a first-order optimization algorithm designed for large-scale unconstrained optimization. Unlike steepest descent, CG generates conjugate directions that lead to faster convergence.

The **Fletcherâ€“Reeves version** of CG:

* Uses gradient vectors and updates the search direction based on previous gradient information
* Does not require Hessian matrix or its approximation
* Is efficient for quadratic or nearly quadratic functions

**Key update rule**:

* `dâ‚€ = -âˆ‡fâ‚€` (initial direction)
* `Î²â‚– = (âˆ‡fâ‚–)áµ€ âˆ‡fâ‚– / (âˆ‡fâ‚–â‚‹â‚)áµ€ âˆ‡fâ‚–â‚‹â‚`
* `dâ‚– = -âˆ‡fâ‚– + Î²â‚– * dâ‚–â‚‹â‚`

## âš™ï¸ Algorithm Steps

1. Initialize `xâ‚€` and set initial direction `dâ‚€ = -âˆ‡fâ‚€`
2. For each iteration:

   * Perform line search to find optimal step size `Î±â‚–`
   * Update position: `xâ‚–â‚Šâ‚ = xâ‚– + Î±â‚– * dâ‚–`
   * Compute new gradient `âˆ‡fâ‚–â‚Šâ‚`
   * Compute `Î²â‚–` using Fletcherâ€“Reeves formula
   * Update direction: `dâ‚– = -âˆ‡fâ‚– + Î²â‚– * dâ‚–â‚‹â‚`
3. Repeat until convergence

---

# KÄ±sÄ±tsÄ±z Optimizasyon iÃ§in Conjugate Gradient (Fletcherâ€“Reeves) AlgoritmasÄ±

Bu repoda, kÄ±sÄ±tsÄ±z doÄŸrusal olmayan optimizasyon problemleri iÃ§in kullanÄ±lan **Conjugate Gradient (Konjugat Gradyan) YÃ¶ntemi**, Ã¶zellikle **Fletcherâ€“Reeves** varyantÄ± uygulanmÄ±ÅŸtÄ±r.

## ğŸ“Œ Problem TanÄ±mÄ±

AmaÃ§, herhangi bir kÄ±sÄ±t olmadan, tÃ¼revlenebilir Ã§ok deÄŸiÅŸkenli bir fonksiyon `f(x)`'in minimumunu bulmaktÄ±r. Bu iÅŸlem yalnÄ±zca birinci dereceden tÃ¼rev (gradyan) bilgisi ile gerÃ§ekleÅŸtirilir.

Matematiksel ifade:

```math
\min f(x), \quad x \in \mathbb{R}^n
```

## ğŸ” YÃ¶ntem Ã–zeti

**Conjugate Gradient (CG) YÃ¶ntemi**, bÃ¼yÃ¼k boyutlu kÄ±sÄ±tsÄ±z optimizasyon problemleri iÃ§in geliÅŸtirilmiÅŸ bir birinci dereceden algoritmadÄ±r. En dik iniÅŸ yÃ¶nteminden farklÄ± olarak, her iterasyonda doÄŸrultular birbirine konjugat olacak ÅŸekilde seÃ§ilir ve bu sayede daha hÄ±zlÄ± yakÄ±nsama saÄŸlanÄ±r.

**Fletcherâ€“Reeves sÃ¼rÃ¼mÃ¼**:

* Gradyan bilgilerini kullanarak arama yÃ¶nÃ¼nÃ¼ gÃ¼nceller
* Hessian matrisine veya yaklaÅŸÄ±k hesaplamasÄ±na gerek duymaz
* Kuadratik veya kuadratiÄŸe yakÄ±n fonksiyonlar iÃ§in oldukÃ§a etkilidir

**Temel gÃ¼ncelleme formÃ¼lÃ¼**:

* `dâ‚€ = -âˆ‡fâ‚€` (ilk yÃ¶n)
* `Î²â‚– = (âˆ‡fâ‚–)áµ€ âˆ‡fâ‚– / (âˆ‡fâ‚–â‚‹â‚)áµ€ âˆ‡fâ‚–â‚‹â‚`
* `dâ‚– = -âˆ‡fâ‚– + Î²â‚– * dâ‚–â‚‹â‚`

## âš™ï¸ Algoritma AdÄ±mlarÄ±

1. BaÅŸlangÄ±Ã§ noktasÄ± `xâ‚€` seÃ§ilir, yÃ¶n `dâ‚€ = -âˆ‡fâ‚€` olarak belirlenir
2. Her iterasyonda:

   * Line search ile uygun `Î±â‚–` adÄ±m bÃ¼yÃ¼klÃ¼ÄŸÃ¼ hesaplanÄ±r
   * Yeni nokta: `xâ‚–â‚Šâ‚ = xâ‚– + Î±â‚– * dâ‚–`
   * Yeni gradyan `âˆ‡fâ‚–â‚Šâ‚` hesaplanÄ±r
   * Fletcherâ€“Reeves formÃ¼lÃ¼yle `Î²â‚–` hesaplanÄ±r
   * Yeni yÃ¶n: `dâ‚– = -âˆ‡fâ‚– + Î²â‚– * dâ‚–â‚‹â‚`
3. YakÄ±nsama saÄŸlanana kadar devam edilir
