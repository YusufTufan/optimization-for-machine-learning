# Modified Newton Algorithm for Nonlinear Optimization

This repository contains a Python implementation of the **Modified Newton Algorithm**, an improved version of the classical Newtonâ€™s method used for unconstrained nonlinear optimization.

## ğŸ“Œ Problem Definition

The objective is to find the local minimum of a twice-differentiable scalar function `f(x)`:

```math
\min_{x \in \mathbb{R}^n} f(x)
````

This method leverages second-order derivative information but applies modifications to improve stability and convergence in practical implementations.

## ğŸ” Method Overview

The classical Newton method uses the inverse of the Hessian matrix for updating the solution. However, in cases where the Hessian is not positive definite, the classical method may fail or converge to a saddle point.

The **Modified Newton Method** addresses this by:

* Adjusting the Hessian to ensure it is positive definite (e.g., by adding a multiple of the identity matrix)
* Ensuring that the search direction is a descent direction

### Update Rule:

```math
x_{k+1} = x_k - \alpha_k H_k^{-1} \nabla f(x_k)
```

Where:

* `H_k` is a modified (positive definite) approximation of the Hessian
* `Î±_k` is step size, typically determined via line search

## âš™ï¸ Algorithm Steps

1. Choose initial point `xâ‚€`
2. Compute gradient `âˆ‡f(xâ‚–)` and Hessian `âˆ‡Â²f(xâ‚–)`
3. Modify the Hessian if itâ€™s not positive definite
4. Solve for the direction `dâ‚– = -H_kâ»Â¹ âˆ‡f(xâ‚–)`
5. Use line search to find step size `Î±â‚–`
6. Update: `xâ‚–â‚Šâ‚ = xâ‚– + Î±â‚– dâ‚–`
7. Repeat until convergence

# DoÄŸrusal Olmayan Optimizasyon iÃ§in DeÄŸiÅŸtirilmiÅŸ Newton (Modified Newton) AlgoritmasÄ±

Bu repoda, klasik Newton yÃ¶nteminin daha kararlÄ± ve uygulanabilir hale getirilmiÅŸ bir versiyonu olan **DeÄŸiÅŸtirilmiÅŸ Newton AlgoritmasÄ±** Python ile uygulanmÄ±ÅŸtÄ±r.

## ğŸ“Œ Problem TanÄ±mÄ±

AmaÃ§, iki kere tÃ¼revlenebilir bir `f(x)` fonksiyonunun yerel minimumunu bulmaktÄ±r:

```math
\min_{x \in \mathbb{R}^n} f(x)
````

YÃ¶ntem, ikinci tÃ¼rev (Hessian) bilgisi kullanarak hÄ±zlÄ± yakÄ±nsama saÄŸlar ancak kararlÄ±lÄ±ÄŸÄ± artÄ±rmak iÃ§in bazÄ± iyileÅŸtirmeler iÃ§erir.

## ğŸ” YÃ¶ntem Ã–zeti

Klasik Newton yÃ¶ntemi, Hessian matrisinin tersini kullanarak Ã§Ã¶zÃ¼m Ã¼retir. Ancak Hessian pozitif tanÄ±mlÄ± deÄŸilse, yÃ¶ntem baÅŸarÄ±sÄ±z olabilir veya eyer noktasÄ±na yakÄ±nsayabilir.

**DeÄŸiÅŸtirilmiÅŸ Newton YÃ¶ntemi**, bu sorunlarÄ± aÅŸaÄŸÄ±daki yollarla Ã§Ã¶zer:

* Hessian matrisini pozitif tanÄ±mlÄ± yapmak iÃ§in gerekirse Ã¼zerine sabit birim matris eklenir
* Arama yÃ¶nÃ¼nÃ¼n iniÅŸ yÃ¶nÃ¼ olmasÄ± garanti altÄ±na alÄ±nÄ±r

### GÃ¼ncelleme KuralÄ±:

```math
x_{k+1} = x_k - \alpha_k H_k^{-1} \nabla f(x_k)
```

Burada:

* `H_k`, pozitif tanÄ±mlÄ± hale getirilmiÅŸ Hessian matrisidir
* `Î±_k`, adÄ±m bÃ¼yÃ¼klÃ¼ÄŸÃ¼dÃ¼r ve genelde Ã§izgisel arama (line search) ile belirlenir

## âš™ï¸ Algoritma AdÄ±mlarÄ±

1. BaÅŸlangÄ±Ã§ noktasÄ± `xâ‚€` seÃ§ilir
2. `âˆ‡f(xâ‚–)` gradyanÄ± ve `âˆ‡Â²f(xâ‚–)` Hessianâ€™Ä± hesaplanÄ±r
3. Hessian pozitif tanÄ±mlÄ± deÄŸilse modifiye edilir
4. YÃ¶n vektÃ¶rÃ¼ Ã§Ã¶zÃ¼lÃ¼r: `dâ‚– = -H_kâ»Â¹ âˆ‡f(xâ‚–)`
5. Ã‡izgisel arama ile uygun `Î±â‚–` bulunur
6. GÃ¼ncelleme yapÄ±lÄ±r: `xâ‚–â‚Šâ‚ = xâ‚– + Î±â‚– dâ‚–`
7. YakÄ±nsama saÄŸlanana kadar devam edilir

