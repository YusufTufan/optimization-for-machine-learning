# Newtonâ€“Raphson Method for Unconstrained Optimization

This repository presents an implementation of the **Newtonâ€“Raphson Algorithm**, a second-order optimization method used to find the local minimum of differentiable scalar functions.

## ğŸ“Œ Problem Definition

The goal is to minimize a twice-differentiable objective function `f(x)` over â„â¿:

```math
\min_{x \in \mathbb{R}^n} f(x)
````

The method utilizes both the gradient and the Hessian matrix to determine the search direction and step size.

## ğŸ” Method Overview

The **Newtonâ€“Raphson method** is a root-finding approach adapted for optimization by setting the gradient to zero:

```math
\nabla f(x) = 0
```

A second-order Taylor expansion of `f(x)` is used to construct a quadratic model around the current point. The next point is then found by solving:

```math
x_{k+1} = x_k - H^{-1} \nabla f(x_k)
```

Where:

* `âˆ‡f(xâ‚–)` is the gradient vector
* `H` is the Hessian matrix of second derivatives

### Key Features:

* Fast local convergence if the Hessian is well-behaved
* Requires second-order derivatives
* Can converge to saddle points if not carefully handled

## âš™ï¸ Algorithm Steps

1. Choose an initial guess `xâ‚€`
2. While stopping criterion not met:

   * Compute gradient `âˆ‡f(xâ‚–)`
   * Compute Hessian matrix `H`
   * Solve: `xâ‚–â‚Šâ‚ = xâ‚– - Hâ»Â¹ âˆ‡f(xâ‚–)`
3. Return estimated minimum point `x`

````

---

```markdown
# KÄ±sÄ±tsÄ±z Optimizasyon iÃ§in Newtonâ€“Raphson YÃ¶ntemi

Bu repoda, iki kere tÃ¼revlenebilir fonksiyonlarÄ±n minimumunu bulmak iÃ§in kullanÄ±lan ikinci dereceden bir yÃ¶ntem olan **Newtonâ€“Raphson AlgoritmasÄ±** uygulanmaktadÄ±r.

## ğŸ“Œ Problem TanÄ±mÄ±

AmaÃ§, aÅŸaÄŸÄ±daki gibi bir `f(x)` fonksiyonunu â„â¿ uzayÄ±nda minimize etmektir:

```math
\min_{x \in \mathbb{R}^n} f(x)
````

YÃ¶ntem, hem gradyan hem de Hessian (ikinci tÃ¼rev matrisini) kullanarak arama yÃ¶nÃ¼nÃ¼ ve adÄ±m bÃ¼yÃ¼klÃ¼ÄŸÃ¼nÃ¼ belirler.

## ğŸ” YÃ¶ntem Ã–zeti

**Newtonâ€“Raphson yÃ¶ntemi**, orijinalde kÃ¶k bulma amacÄ±yla geliÅŸtirilmiÅŸtir. Optimizasyonda, `âˆ‡f(x) = 0` eÅŸitliÄŸi Ã§Ã¶zÃ¼lerek lokal minimum noktasÄ± aranÄ±r.

Fonksiyonun ikinci dereceden Taylor aÃ§Ä±lÄ±mÄ± ile Ã§evresindeki yaklaÅŸÄ±k model kurulur. Sonraki nokta ÅŸu ÅŸekilde hesaplanÄ±r:

```math
x_{k+1} = x_k - H^{-1} \nabla f(x_k)
```

Burada:

* `âˆ‡f(xâ‚–)` gradyan vektÃ¶rÃ¼dÃ¼r
* `H`, fonksiyonun Hessian matrisidir (ikinci tÃ¼revler)

### Temel Ã–zellikler:

* EÄŸer Hessian dÃ¼zgÃ¼nse hÄ±zlÄ± yakÄ±nsama saÄŸlar
* Ä°kinci tÃ¼rev bilgisi gerektirir
* Uygun kontrol yapÄ±lmazsa eyer noktalarÄ±na yakÄ±nsama riski vardÄ±r

## âš™ï¸ Algoritma AdÄ±mlarÄ±

1. BaÅŸlangÄ±Ã§ noktasÄ± `xâ‚€` seÃ§ilir
2. Durma koÅŸulu saÄŸlanana kadar:

   * Gradyan `âˆ‡f(xâ‚–)` hesaplanÄ±r
   * Hessian matrisi `H` bulunur
   * Yeni nokta hesaplanÄ±r: `xâ‚–â‚Šâ‚ = xâ‚– - Hâ»Â¹ âˆ‡f(xâ‚–)`
3. Minimum noktasÄ± `x` dÃ¶ndÃ¼rÃ¼lÃ¼r
